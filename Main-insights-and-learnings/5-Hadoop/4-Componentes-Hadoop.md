# Componentes y Ecosistema de Hadoop

Hadoop es un ecosistema de Big Data que consta de varios componentes dise침ados para el procesamiento y almacenamiento de grandes vol칰menes de datos en cl칰steres de servidores. Dos de los componentes clave son el ***Sistema de Ficheros Distribuido de Hadoop (HDFS)*** y el entorno de programaci칩n ***MapReduce***.

Sin embargo, Hadoop se complementa con otras herramientas que desempe침an un papel importante en la ingesti칩n y transferencia de datos.

## Componentes del n칰cleo de Hadoop 游빌

### Hadoop Common

Conjunto de utilidades comunes.

### HDFS (Hadoop Distributed File System)

HDFS es el sistema de almacenamiento distribuido de Hadoop, dise침ado para el manejo de grandes conjuntos de datos. Los datos se dividen en bloques y se almacenan en **nodos distribuidos en el cl칰ster**, lo que permite un *alto grado de redundancia y tolerancia a fallos*. HDFS es esencial para el almacenamiento escalable y confiable de datos en Hadoop.

### MapReduce

Modelo de programaci칩n y procesamiento de datos en Hadoop. Basado en el *principio de dividir y conquistar*, MapReduce divide el proceso de an치lisis en etapas de "map" y "reduce", lo que permite el **procesamiento en paralelo** en varios nodos del cl칰ster.

![Map Reduce Proceso](/Main-insights-and-learnings/5-Hadoop/img/map-reduce.png)

### Yarn (Yet Another Resource Negotiator)

YARN es un administrador de recursos y programaci칩n de trabajos en cl칰steres. Permite la ejecuci칩n de aplicaciones m치s all치 de MapReduce, como Spark y Hive.

![Map Reduce Proceso](/Main-insights-and-learnings/5-Hadoop/img/yarnArquitectura.png)

## Otros componentes 游빌

Adem치s de las anteriores, Hadoop tiene un vasto ecosistema de herramientas que simplifican el acceso, gesti칩n y expansi칩n de su funcionalidad. Las m치s utilizadas son:

### Hive

[Hive](https://hive.apache.org/index.html) es una herramienta de almacenamiento y consulta de datos basada en SQL dise침ada para simplificar la gesti칩n y el an치lisis de grandes conjuntos de datos en el entorno de Hadoop.

Permite acceder a HDFS como si fuera una Base de datos, ejecutando comandos muy parecidos a SQL para recuperar valores *(HiveSQL)*.

Aunque ofrece ***ventajas*** como la familiaridad del SQL y la reducci칩n de la complejidad de MapReduce, tambi칠n tiene ****limitaciones***, como no ser la mejor opci칩n para consultas en tiempo real y tener un soporte SQL m치s limitado en comparaci칩n con bases de datos relacionales tradicionales.

### HBase

[Apache HBase](https://hbase.apache.org/) es una base de datos NoSQL altamente escalable y distribuida que se utiliza en aplicaciones que requieren un ***acceso en tiempo real a grandes vol칰menes de datos***. Su dise침o permite un alto rendimiento y alta disponibilidad, lo que lo hace adecuado para una variedad de casos de uso, como aplicaciones web, an치lisis de registros, sistemas de recomendaci칩n y m치s.

### Spark

[Spark](https://spark.apache.org/) es un marco de procesamiento de datos en memoria que puede ser m치s r치pido que MapReduce para ciertos tipos de trabajos. Puede interactuar bien con HDFS y se utiliza ampliamente para an치lisis de datos en tiempo real y procesamiento de lotes.

En vez de almacenar los datos en disco, trabaja de forma masiva en memoria

### Pig

[Pig](https://pig.apache.org/) es un lenguaje de *alto nivel* para escribir programas que se ejecutan en Hadoop. Permite el procesamiento de datos en Hadoop en una forma m치s program치tica y es 칰til para tareas de ETL (Extract, Transform, Load).

Realmente es un **compilador** que genera comandos MapReduce, mediante el lenguaje textual denominado *Pig Latin*.

### Sqoop

[Sqoop](https://sqoop.apache.org/) es una herramienta dise침ada para la importaci칩n y exportaci칩n de datos entre bases de datos relacionales y Hadoop. Su objetivo es *facilitar la transferencia de datos* entre sistemas de almacenamiento estructurados y HDFS. Sqoop es especialmente 칰til para mover datos desde bases de datos relacionales a Hadoop para su an치lisis y procesamiento.

### Flume

[Flume](https://flume.apache.org/) es una herramienta de ingesti칩n de datos dise침ada para la transferencia confiable y escalable de datos en tiempo real desde diversas fuentes a Hadoop HDFS u otros destinos. 

Se utiliza para **recopilar, agregar y mover datos en tiempo real** desde una variedad de fuentes de flujo continuo, como logs de aplicaciones y eventos. Utiliza una arquitectura de tipo streaming con un flujo de datos muy potente y personalizables.

#### Componentes:

1. **Spark Core**:
   - Es el coraz칩n de Apache Spark y proporciona las funcionalidades fundamentales del sistema, como el manejo de la distribuci칩n de datos, la tolerancia a fallos y la programaci칩n de trabajos en paralelo.
   - Contiene las API para la manipulaci칩n de datos en forma de Resilient Distributed Datasets (RDD), que son colecciones inmutables y distribuidas de datos.

2. **Spark SQL**:
   - Es un m칩dulo que permite realizar consultas estructuradas utilizando SQL en Spark. Puede procesar datos estructurados y semiestructurados desde una variedad de fuentes, como DataFrames, conjuntos de datos JSON, parquet y m치s.
   - Facilita la integraci칩n de Spark con fuentes de datos externas, como bases de datos relacionales y archivos CSV.

3. **Spark Streaming**:
   - Permite el procesamiento en tiempo real de datos de transmisi칩n, como flujos de eventos y registros. Puede conectarse a fuentes de datos en tiempo real como Kafka, Flume y m치s.
   - Transforma y analiza datos de transmisi칩n en intervalos peque침os y predefinidos para aplicaciones de procesamiento en tiempo real.

4. **MLlib (Machine Learning Library)**:
   - Biblioteca de aprendizaje autom치tico de Spark. Ofrece una amplia gama de algoritmos y herramientas para el aprendizaje supervisado y no supervisado, clasificaci칩n, regresi칩n y m치s.
   - Facilita la construcci칩n de modelos de aprendizaje autom치tico a gran escala en cl칰steres de Spark.

5. **GraphX**:
   - Biblioteca de an치lisis de grafos integrada en Spark. Permite el procesamiento y an치lisis de datos gr치ficos en grandes conjuntos de datos.
   - Se utiliza para realizar operaciones de gr치ficos, como recorridos de grafos y an치lisis de componentes conectados.

6. **SparkR**:
   - Interfaz de R para Spark que permite a los usuarios de R aprovechar el poder de procesamiento de Spark. Permite ejecutar an치lisis estad칤sticos y de aprendizaje autom치tico en cl칰steres de Spark utilizando el lenguaje R.


Estos componentes, junto con otros proyectos y herramientas en el ecosistema Hadoop, permiten la gesti칩n eficiente de grandes vol칰menes de datos y el an치lisis de datos a gran escala en entornos distribuidos. La elecci칩n de componentes espec칤ficos depende de los requisitos de tu caso de uso y de la naturaleza de los datos que debas procesar.