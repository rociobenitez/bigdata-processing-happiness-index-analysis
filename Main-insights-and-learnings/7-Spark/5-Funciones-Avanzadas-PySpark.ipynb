{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones en PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook aprenderemos algunas funciones avanzadas para optimizar el rendimiento de Spark, para imputar valores faltantes o a crear funciones definidas por el usuario (UDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crea la sesión de SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/09 10:52:48 WARN Utils: Your hostname, MacBook-Air-de-Rocio.local resolves to a loopback address: 127.0.0.1; using 192.168.1.42 instead (on interface en0)\n",
      "23/11/09 10:52:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/09 10:52:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/11/09 10:52:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear el DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/09 10:53:01 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "emp = [(1, \"AAA\", \"dept1\", 1000),\n",
    "    (2, \"BBB\", \"dept1\", 1100),\n",
    "    (3, \"CCC\", \"dept1\", 3000),\n",
    "    (4, \"DDD\", \"dept1\", 1500),\n",
    "    (5, \"EEE\", \"dept2\", 8000),\n",
    "    (6, \"FFF\", \"dept2\", 7200),\n",
    "    (7, \"GGG\", \"dept3\", 7100),\n",
    "    (None, None, None, 7500),\n",
    "    (9, \"III\", None, 4500),\n",
    "    (10, None, \"dept5\", 2500)]\n",
    "\n",
    "dept = [(\"dept1\", \"Department - 1\"),\n",
    "        (\"dept2\", \"Department - 2\"),\n",
    "        (\"dept3\", \"Department - 3\"),\n",
    "        (\"dept4\", \"Department - 4\")\n",
    "       ]\n",
    "\n",
    "df = spark.createDataFrame(emp, [\"id\", \"name\", \"dept\", \"salary\"])\n",
    "deptdf = spark.createDataFrame(dept, [\"id\", \"name\"]) \n",
    "\n",
    "# Create Temp Tables\n",
    "df.createOrReplaceTempView(\"empdf\")\n",
    "deptdf.createOrReplaceTempView(\"deptdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Expresiones SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos usar la expresión SQL para la manipulación de datos. Tenemos la función **expr** y también una variante de un método de selección como **selectExpr** para la evaluación de expresiones SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+------------+\n",
      "|  id|name| dept|salary|salary_level|\n",
      "+----+----+-----+------+------------+\n",
      "|   1| AAA|dept1|  1000|  low_salary|\n",
      "|   2| BBB|dept1|  1100|  low_salary|\n",
      "|   3| CCC|dept1|  3000|  mid_salary|\n",
      "|   4| DDD|dept1|  1500|  low_salary|\n",
      "|   5| EEE|dept2|  8000| high_salary|\n",
      "|   6| FFF|dept2|  7200| high_salary|\n",
      "|   7| GGG|dept3|  7100| high_salary|\n",
      "|NULL|NULL| NULL|  7500| high_salary|\n",
      "|   9| III| NULL|  4500|  mid_salary|\n",
      "|  10|NULL|dept5|  2500|  mid_salary|\n",
      "+----+----+-----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Intentemos categorizar el salario en Bajo, Medio y Alto según la categorización a continuación.\n",
    "\n",
    "# 0-2000: salario_bajo\n",
    "# 2001 - 5000: mid_salary\n",
    "#> 5001: high_salary\n",
    "\n",
    "cond = \"\"\"case when salary > 5000 then 'high_salary'\n",
    "               else case when salary > 2000 then 'mid_salary'\n",
    "                    else case when salary > 0 then 'low_salary'\n",
    "                         else 'invalid_salary'\n",
    "                              end\n",
    "                         end\n",
    "                end as salary_level\"\"\"\n",
    "\n",
    "newdf = df.withColumn(\"salary_level\", expr(cond))\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando la función selectExpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+------------+\n",
      "|  id|name| dept|salary|salary_level|\n",
      "+----+----+-----+------+------------+\n",
      "|   1| AAA|dept1|  1000|  low_salary|\n",
      "|   2| BBB|dept1|  1100|  low_salary|\n",
      "|   3| CCC|dept1|  3000|  mid_salary|\n",
      "|   4| DDD|dept1|  1500|  low_salary|\n",
      "|   5| EEE|dept2|  8000| high_salary|\n",
      "|   6| FFF|dept2|  7200| high_salary|\n",
      "|   7| GGG|dept3|  7100| high_salary|\n",
      "|NULL|NULL| NULL|  7500| high_salary|\n",
      "|   9| III| NULL|  4500|  mid_salary|\n",
      "|  10|NULL|dept5|  2500|  mid_salary|\n",
      "+----+----+-----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.selectExpr(\"*\", cond)\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones definidas por el usuario (UDF)\n",
    "A menudo necesitamos escribir la función en función de nuestro requisito muy específico. Aquí podemos aprovechar las udfs. Podemos escribir nuestras propias funciones en un lenguaje como python y registrar la función como udf, luego podemos usar la función para operaciones de DataFrame.\n",
    "\n",
    "* Función de Python para encontrar el nivel_salario para un salario dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detSalary_Level(sal):\n",
    "    level = None\n",
    "\n",
    "    if(sal > 5000):\n",
    "        level = 'high_salary'\n",
    "    elif(sal > 2000):\n",
    "        level = 'mid_salary'\n",
    "    elif(sal > 0):\n",
    "        level = 'low_salary'\n",
    "    else:\n",
    "        level = 'invalid_salary'\n",
    "    return level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Luego registre la función \"detSalary_Level\" como UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal_level = udf(detSalary_Level, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Aplicar función para determinar el salario_level para un salario dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 6:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+------------+\n",
      "|  id|name| dept|salary|salary_level|\n",
      "+----+----+-----+------+------------+\n",
      "|   1| AAA|dept1|  1000|  low_salary|\n",
      "|   2| BBB|dept1|  1100|  low_salary|\n",
      "|   3| CCC|dept1|  3000|  mid_salary|\n",
      "|   4| DDD|dept1|  1500|  low_salary|\n",
      "|   5| EEE|dept2|  8000| high_salary|\n",
      "|   6| FFF|dept2|  7200| high_salary|\n",
      "|   7| GGG|dept3|  7100| high_salary|\n",
      "|NULL|NULL| NULL|  7500| high_salary|\n",
      "|   9| III| NULL|  4500|  mid_salary|\n",
      "|  10|NULL|dept5|  2500|  mid_salary|\n",
      "+----+----+-----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.withColumn(\"salary_level\", sal_level(\"salary\"))\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabajando con valores NULL\n",
    "\n",
    "Los valores NULL siempre son difíciles de manejar independientemente del Framework o lenguaje que usemos. Aquí en Spark tenemos pocas funciones específicas para lidiar con valores NULL.\n",
    "\n",
    "- **es nulo()**\n",
    "\n",
    "Esta función nos ayudará a encontrar los valores nulos para cualquier columna dada. Por ejemplo si necesitamos encontrar las columnas donde las columnas id contienen los valores nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+------+\n",
      "|  id|name|dept|salary|\n",
      "+----+----+----+------+\n",
      "|NULL|NULL|NULL|  7500|\n",
      "|   9| III|NULL|  4500|\n",
      "+----+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.filter(df[\"dept\"].isNull())\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **No es nulo()**\n",
    "\n",
    "Esta función funciona de manera opuesta a la función isNull () y devolverá todos los valores no nulos para una función en particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "|  2| BBB|dept1|  1100|\n",
      "|  3| CCC|dept1|  3000|\n",
      "|  4| DDD|dept1|  1500|\n",
      "|  5| EEE|dept2|  8000|\n",
      "|  6| FFF|dept2|  7200|\n",
      "|  7| GGG|dept3|  7100|\n",
      "| 10|NULL|dept5|  2500|\n",
      "+---+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.filter(df[\"dept\"].isNotNull())\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **fillna ()**\n",
    "\n",
    "Esta función nos ayudará a reemplazar los valores nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------+------+\n",
      "|  id|name|   dept|salary|\n",
      "+----+----+-------+------+\n",
      "|   1| AAA|  dept1|  1000|\n",
      "|   2| BBB|  dept1|  1100|\n",
      "|   3| CCC|  dept1|  3000|\n",
      "|   4| DDD|  dept1|  1500|\n",
      "|   5| EEE|  dept2|  8000|\n",
      "|   6| FFF|  dept2|  7200|\n",
      "|   7| GGG|  dept3|  7100|\n",
      "|NULL|NULL|INVALID|  7500|\n",
      "|   9| III|INVALID|  4500|\n",
      "|  10|NULL|  dept5|  2500|\n",
      "+----+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace -1 where the salary is null.\n",
    "newdf = df.fillna(\"INVALID\", [\"dept\"])\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **dropna ()**\n",
    "\n",
    "Esta función nos ayudará a eliminar las filas con valores nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n",
      "| id|name| dept|salary|\n",
      "+---+----+-----+------+\n",
      "|  1| AAA|dept1|  1000|\n",
      "|  2| BBB|dept1|  1100|\n",
      "|  3| CCC|dept1|  3000|\n",
      "|  4| DDD|dept1|  1500|\n",
      "|  5| EEE|dept2|  8000|\n",
      "|  6| FFF|dept2|  7200|\n",
      "|  7| GGG|dept3|  7100|\n",
      "+---+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove all rows which contains any null values.\n",
    "newdf = df.dropna()\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning\n",
    "\n",
    "\n",
    "El particionamiento es un aspecto muy importante para controlar el paralelismo de la aplicación Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Consultar número de particiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Incrementar el número de particiones. Por ejemplo, aumentar las particiones a 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf = df.repartition(6)\n",
    "newdf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota: se trata de operaciones costosas, ya que requiere la mezcla de datos entre los trabajadores.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Disminuir el número de particiones. Por ejemplo disminuir las particiones a 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf = df.coalesce(2)\n",
    "newdf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* De forma predeterminada, el número de particiones para Spark SQL es 200.\n",
    "* Pero también podemos establecer el número de particiones en el nivel de aplicación Spark. Por ejemplo establecido en 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Partitions : 500\n"
     ]
    }
   ],
   "source": [
    "# Set number of partitions as Spark Application.\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"500\")\n",
    "\n",
    "# Check the number of patitions.\n",
    "num_part = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "print(\"No of Partitions : {0}\".format(num_part))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
